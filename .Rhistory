layerSizes <- c(2,4,2)
actFuncByLayer <- list(c(sigmoid, sigmoid, relu, relu), c(linear, linear))
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- matrix(runif(layerSizes[1]),ncol=1)
dataOutputs <- matrix(data = 1:layerSizes[length(layerSizes)],ncol = 1)
x <- weightsByLayerToX(weightsByLayer = Weights)
squareLoss(x)
# derivative of the square loss function, 1/2 ||pred-y||^2 , w.r.t. pred
# i.e.,  partial_Loss / partial_net_output
squareLossDeriv <- function(pred,y){
return(t(pred-y))
}
# backpropagation function with only 1 data point.
# This routine is NOT optimized for performance (growing lists A and Z are inefficient, ...)
backPropagation <- function(dataIn, dataOut, weightsByLayer, actFunc, actFuncDeriv, lossDeriv=squareLossDeriv) {
# adding an identity last layer (+row of 0's to implement the bias) to get simpler recursions
nOutLayer <- nrow(dataOut)
weightsByLayer[[length(weightsByLayer)+1]]<-rbind(diag(x = rep(1,nOutLayer)),rep(0,nOutLayer))
numberOfLayers <- length(weightsByLayer)
A <- vector(mode = "list", numberOfLayers)
Z <- vector(mode = "list",numberOfLayers)
# first, forward propagation to calculate the network state, cf. forwardPropagation function
for(i in 1:numberOfLayers) {
if (i==1) {A[[i]]<-rbind(dataIn,1)}
else {
for (j in 1:length(actFunc[[i-1]])) { # for each neuron of the layer
A[[i]][j] <- actFunc[[i-1]][[j]](Z[[i-1]][j])
}
A[[i]] <- rbind(as.matrix(A[[i]]), 1) # for the bias
}
Z[[i]] <- t(weightsByLayer[[i]]) %*% A[[i]]
}
# backward loop
delta <- lossDeriv(pred=Z[[numberOfLayers]],y=dataOut)
dloss_dweight <- vector(mode = "list",numberOfLayers-1) # the derivatives are first stored in the same format as the weights
for (i in numberOfLayers:2) {
# delta(l-1) = delta(l) * dz(l)_da(l) * da(l)_dz(l-1)
# where delta(l) = dloss_dz(l)
# da(l)_dz(l-1) is diagonal (neurons within a layer are not connected to each other)
#   with an added row of 0's corresponding to the constant a(l)_last=1 (our implementation of the biases)
Nbneurons <- length(Z[[i-1]])
D <- rep(x = NA,Nbneurons)
for (j in 1:Nbneurons){
D[j]<-actFuncDeriv[[i-1]][[j]](Z[[i-1]][j])
}
delta <- delta %*% t(weightsByLayer[[i]]) %*% rbind(diag(D),rep(0,Nbneurons))
# dloss_dw(l) = a(l)*delta(l)
dloss_dweight[[i-1]]<-A[[i-1]] %*% delta
}
# derivatives now stored in a vector where each weight matrix is visited by column in the order of the layers
dloss_dtheta <- NULL
for (i in 1:(numberOfLayers-1)){
dloss_dtheta <- c(dloss_dtheta,dloss_dweight[[i]])
}
return(list(pred=Z[[numberOfLayers]],dloss_dtheta=dloss_dtheta))
}
layerSizes <- c(2,3,4,2)
actFuncByLayer <- list(c(sigmoid, sigmoid, relu),
c(sigmoid,sigmoid,relu,relu),
c(linear, linear))
DerivActFunc <- list(c(sigmoidDeriv, sigmoidDeriv, reluDeriv),
c(sigmoidDeriv,sigmoidDeriv,reluDeriv,reluDeriv),
c(linearDeriv, linearDeriv))
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- matrix(runif(layerSizes[1]),ncol=1)
dataOutputs <- matrix(data = 1:layerSizes[length(layerSizes)],ncol = 1)
BP <- backPropagation(dataIn=dataInputs, dataOut=dataOutputs, weightsByLayer=Weights, actFunc = actFuncByLayer, actFuncDeriv = DerivActFunc)
# forwardPropagation(inputs=dataInputs, weightsByLayer=Weights, activationFunctionsByLayer=actFuncByLayer)
# function to calculate f and gradient of f by forward finite difference
f.gradf <- function(x,f,h=1.e-8){
d<-length(x)
res <- list()
res$fofx <- f(x)
res$gradf <- rep(NA,d)
for (i in 1:d){
xp <- x
xp[i] <- x[i]+h
res$gradf[i] <- (f(xp)-res$fofx)/h
}
return(res)
}
x <- weightsByLayerToX(weightsByLayer = Weights)
# do a finite difference at x with squareLoss function
fdiff <- f.gradf(x=x,f=squareLoss)
# compare with the backpropagation value (should be close to 0)
BP$dloss_dtheta-fdiff$gradf
source('test_functions.R')
# normalization routines
normByRow <- function(X){
nr <- dim(X)[1]
nc <- dim(X)[2]
Xnorm <- matrix(nrow = nr,ncol = nc)
minAndMax <- matrix(nrow = nc,ncol=2)
for (i in 1:nc){
zmin<-min(X[,i])
zmax<-max(X[,i])
minAndMax[i,]<-c(zmin,zmax)
Xnorm[,i]<-2*(X[,i]-zmin)/(zmax-zmin)-1
}
res<-list()
res$dat <- Xnorm
res$minAndMax <- minAndMax
return(res)
}
unnormByRow <- function(normDat){
nr <- dim(normDat$dat)[1]
nc <- dim(normDat$dat)[2]
X <- matrix(nrow = nr,ncol = nc)
for (i in 1:nc){
zmin<-normDat$minAndMax[i,1]
zmax<-normDat$minAndMax[i,2]
X[,i]<-(normDat$dat[,i]+1)/2*(zmax-zmin)+zmin
}
return(X)
}
fun<-quadratic
d<-2
LB <- rep(-5,d)
UB <- rep(5,d)
ntrain <- 15
ntest <- 100
ndata <- ntrain + ntest
set.seed(1) # with this seed you can reproduce the data
rawX <-t(replicate(n = ndata,expr = runif(n = d,min = LB,max = UB)))
set.seed(Sys.time()) # unset seed, back to "random"
rawYobs <- apply(X = rawX,MARGIN = 1,FUN = fun)
# normalize the data between -1 and 1
X <- normByRow(rawX)
# you can recover unnormalized data with, for expl : X <- unnormByRow(normIn)
Yobs <- normByRow(as.matrix(rawYobs))
itrain <- 1:ntrain
itest <- (ntrain+1):ndata
# data in stats are stored as 1 row for 1 point.
# In the neural network world, like often in linear algebra, vectors are columns thus a transpose is needed
Xtrain<-t(X$dat[itrain,])
Xtest<-t(X$dat[itest,])
Ytrain<-t(matrix(data=Yobs$dat[itrain,],ncol=1))
Ytest<-t(matrix(data=Yobs$dat[itest,],ncol=1))
# Example of loss calculation
layerSizes <- c(2,4,1)
actFuncByLayer <- list(c(sigmoid, sigmoid, relu, relu), c(linear))
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
squareLoss(x)
source('utilities_optim.R')
source('line_searches.R')
source('gradient_descent.R')
source('restarted_descent.R')
pbFormulation <- list()
pbFormulation$fun<-squareLoss #function to minimize
d<-length(x)
pbFormulation$d<-d # dimension
pbFormulation$LB<-rep(-10,d) #lower bounds
pbFormulation$UB<-rep(10,d) #upper bounds
### algorithm settings
optAlgoParam <- list()
set.seed(112233) # repeatable run despite the use of pseudo-random number generators in runif
#
optAlgoParam$budget <- 4000
optAlgoParam$minGradNorm <- 1.e-6
optAlgoParam$minStepSize <- 1.e-11
optAlgoParam$nb_restarts <- 4
#
optAlgoParam$direction_type <- "momentum"
optAlgoParam$linesearch_type <- "armijo"
optAlgoParam$stepFactor <- 0.1 # step factor when there is no line search,
optAlgoParam$beta <- 0.9 # momentum term for direction_type == "momentum" or "NAG"
optAlgoParam$xinit <- weightsByLayerToX(weightsByLayer = Weights)
#
printlevel <- 2 # controls how much is stored and printed, choices: 0 to 4, cf. gradient_descent.R top comments for more info.
# a restarted descent
res <- restarted_descent(pbFormulation=pbFormulation,algoParam = optAlgoParam,printlevel=printlevel)
# extract the net learned
xBest <- res$xbest
lossBest <- res$fbest
wBest <- xtoWeightsByLayer(xBest,layerSizes)
# Performance on the training set
predTrain <- forwardPropagation(Xtrain, wBest, actFuncByLayer) # predictions of the NN learned
rmseTrain <- sqrt((sum((Ytrain-predTrain)^2))/ntrain)
cat("Training RMSE =",rmseTrain,"\n")
plot(Ytrain,predTrain,xlab="data",ylab="prediction",main="training",pch=3,col="blue")
ymin <- min(Ytrain)
ymax <- max(Ytrain)
lines(x = c(ymin,ymax), y=c(ymin,ymax))
# Performance on the testing set
predTest <- forwardPropagation(Xtest, wBest, actFuncByLayer) # predictions of the NN learned
rmseTest <- sqrt((sum((Ytest-predTest)^2))/ntest)
cat("Test RMSE =",rmseTest,"\n")
plot(Ytest,predTest,xlab="data",ylab="prediction",main="test",pch=3,col="red")
ymin <- min(Ytest)
ymax <- max(Ytest)
lines(x = c(ymin,ymax), y=c(ymin,ymax))
optAlgoParam$direction_type <- "momentum"
optAlgoParam$linesearch_type <- "armijo"
optAlgoParam$stepFactor <- 1 # step factor when there is no line search,
optAlgoParam$beta <- 0.5 # momentum term for direction_type == "momentum" or "NAG"
# a gradient descent
res_grad <- gradient_descent(pbFormulation=pbFormulation,algoParam = optAlgoParam,printlevel=printlevel)
# extract the net learned
xBest_grad <- res_grad$xbest
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad,layerSizes)
wBest_grad
# Performance sur l'ensemble d'entraînement
predTrain <- forwardPropagation(Xtrain, wBest_grad, actFuncByLayer) # Prédictions du réseau appris
rmseTrain <- sqrt((sum((Ytrain - predTrain)^2)) / ntrain)
cat("RMSE Entraînement =", rmseTrain, "\n")
plot(Ytrain, predTrain, xlab = "Données", ylab = "Prédictions", main = "Entraînement", pch = 3, col = "blue")
ymin <- min(Ytrain)
ymax <- max(Ytrain)
lines(x = c(ymin, ymax), y = c(ymin, ymax))
# Performance sur l'ensemble de test
predTest <- forwardPropagation(Xtest, wBest_grad, actFuncByLayer)
# Prédictions du réseau appris
rmseTest <- sqrt((sum((Ytest - predTest)^2)) / ntest)
cat("RMSE Test =", rmseTest, "\n")
plot(Ytest, predTest, xlab = "Données", ylab = "Prédictions", main = "Test", pch = 3, col = "red")
ymin <- min(Ytest)
ymax <- max(Ytest)
lines(x = c(ymin, ymax), y = c(ymin, ymax))
squareLoss<- function(x) {
weights <- xtoWeightsByLayer(x, layerSizes = layerSizes)
pred <- forwardPropagation(inputs = dataInputs, weightsByLayer = weights, activationFunction = actFuncByLayer)
# Assurer que les prédictions sont comprises entre un petit epsilon et 1 - epsilon pour éviter le log(0)
epsilon <- 1e-10
pred <- pmax(pmin(pred, 1 - epsilon), epsilon)
# Calcul de la cross-entropie
return(-mean(dataOutputs * log(pred) + (1 - dataOutputs) * log(1 - pred)))
}
threshold <- 0.5  # This is an example threshold
# Apply the rule
Ytrain <- ifelse(abs(Xtrain) > threshold, 1, 0)
Ytest <- ifelse(abs(Xtest) > threshold, 1, 0)
# Convert to the same format as your original Ytrain and Ytest
Ytrain <- t(matrix(data=Ytrain, ncol=1))
Ytest <- t(matrix(data=Ytest, ncol=1))
Ytrain <- Ytrain[, 1:15, drop = FALSE]
Ytest <- Ytest[, 1:100, drop = FALSE]
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(sigmoid, sigmoid, relu, relu,), c(sigmoid) )
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(sigmoid, sigmoid, relu, relu), c(sigmoid) )
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.01
squareLoss(x)
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=printlevel)
# Extraction du réseau appris
xBest_grad <- res_grad$xbest
print(xBest_grad)
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
evaluate_performance_class <- function(X, Y, weights, actFunc) {
pred <- forwardPropagation(X, weights, actFunc)
predictedClasses <- ifelse(pred > 0.5, 1, 0)  # Convertir les prédictions en classes 0 ou 1
accuracy <- sum(predictedClasses == Y) / length(Y)  # Calcul de la précision
print(predictedClasses)
return(accuracy)
}
# Évaluation de la performance sur l'ensemble d'entraînement
accuracyTrain <- evaluate_performance_class(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
cat("Précision Entraînement =", accuracyTrain, "\n")
# Évaluation de la performance sur l'ensemble de test
accuracyTest <- evaluate_performance_class(Xtest, Ytest, wBest_grad, actFuncByLayer)
cat("Précision Test =", accuracyTest, "\n")
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(sigmoid, sigmoid, relu, relu), c(sigmoid) )
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=printlevel)
# Extraction du réseau appris
xBest_grad <- res_grad$xbest
print(xBest_grad)
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
evaluate_performance_class <- function(X, Y, weights, actFunc) {
pred <- forwardPropagation(X, weights, actFunc)
predictedClasses <- ifelse(pred > 0.5, 1, 0)  # Convertir les prédictions en classes 0 ou 1
accuracy <- sum(predictedClasses == Y) / length(Y)  # Calcul de la précision
print("trueClasses", Y)
print("predictedClasses", predictedClasses)
return(accuracy)
}
# Évaluation de la performance sur l'ensemble d'entraînement
accuracyTrain <- evaluate_performance_class(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=printlevel)
# Extraction du réseau appris
xBest_grad <- res_grad$xbest
print(xBest_grad)
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
evaluate_performance_class <- function(X, Y, weights, actFunc) {
pred <- forwardPropagation(X, weights, actFunc)
predictedClasses <- ifelse(pred > 0.5, 1, 0)  # Convertir les prédictions en classes 0 ou 1
accuracy <- sum(predictedClasses == Y) / length(Y)  # Calcul de la précision
print("trueClasses")
print(Y)
print("predictedClasses")
print(predictedClasses)
return(accuracy)
}
# Évaluation de la performance sur l'ensemble d'entraînement
accuracyTrain <- evaluate_performance_class(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
cat("Précision Entraînement =", accuracyTrain, "\n")
# Évaluation de la performance sur l'ensemble de test
accuracyTest <- evaluate_performance_class(Xtest, Ytest, wBest_grad, actFuncByLayer)
cat("Précision Test =", accuracyTest, "\n")
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(relu, relu, relu, relu), c(sigmoid) )
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=printlevel)
# Extraction du réseau appris
xBest_grad <- res_grad$xbest
print(xBest_grad)
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
evaluate_performance_class <- function(X, Y, weights, actFunc) {
pred <- forwardPropagation(X, weights, actFunc)
predictedClasses <- ifelse(pred > 0.5, 1, 0)  # Convertir les prédictions en classes 0 ou 1
accuracy <- sum(predictedClasses == Y) / length(Y)  # Calcul de la précision
print("trueClasses")
print(Y)
print("predictedClasses")
print(predictedClasses)
return(accuracy)
}
# Évaluation de la performance sur l'ensemble d'entraînement
accuracyTrain <- evaluate_performance_class(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
cat("Précision Entraînement =", accuracyTrain, "\n")
# Évaluation de la performance sur l'ensemble de test
accuracyTest <- evaluate_performance_class(Xtest, Ytest, wBest_grad, actFuncByLayer)
cat("Précision Test =", accuracyTest, "\n")
layerSizes <- c(2, 6, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(relu, relu,relu, relu, relu, relu), c(sigmoid) )
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=printlevel)
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=printlevel)
layerSizes <- c(2, 6, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(relu, relu,relu, relu, relu, relu), c(sigmoid) )
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=printlevel)
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=2)
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(relu, relu, relu, relu), c(sigmoid) )
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=2)
# Extraction du réseau appris
xBest_grad <- res_grad$xbest
print(xBest_grad)
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
evaluate_performance_class <- function(X, Y, weights, actFunc) {
pred <- forwardPropagation(X, weights, actFunc)
predictedClasses <- ifelse(pred > 0.5, 1, 0)  # Convertir les prédictions en classes 0 ou 1
accuracy <- sum(predictedClasses == Y) / length(Y)  # Calcul de la précision
print("trueClasses")
print(Y)
print("predictedClasses")
print(predictedClasses)
return(accuracy)
}
# Évaluation de la performance sur l'ensemble d'entraînement
accuracyTrain <- evaluate_performance_class(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
cat("Précision Entraînement =", accuracyTrain, "\n")
# Évaluation de la performance sur l'ensemble de test
accuracyTest <- evaluate_performance_class(Xtest, Ytest, wBest_grad, actFuncByLayer)
cat("Précision Test =", accuracyTest, "\n")
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c[relu, tanh, relu, tanh], c(sigmoid) )
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(relu, tanh, relu, relu), c(sigmoid) )
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(relu, tanh, relu, tanh), c(sigmoid) )
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=2)
# Extraction du réseau appris
xBest_grad <- res_grad$xbest
print(xBest_grad)
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
evaluate_performance_class <- function(X, Y, weights, actFunc) {
pred <- forwardPropagation(X, weights, actFunc)
predictedClasses <- ifelse(pred > 0.5, 1, 0)  # Convertir les prédictions en classes 0 ou 1
accuracy <- sum(predictedClasses == Y) / length(Y)  # Calcul de la précision
print("trueClasses")
print(Y)
print("predictedClasses")
print(predictedClasses)
return(accuracy)
}
# Évaluation de la performance sur l'ensemble d'entraînement
accuracyTrain <- evaluate_performance_class(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
cat("Précision Entraînement =", accuracyTrain, "\n")
# Évaluation de la performance sur l'ensemble de test
accuracyTest <- evaluate_performance_class(Xtest, Ytest, wBest_grad, actFuncByLayer)
cat("Précision Test =", accuracyTest, "\n")
q()
