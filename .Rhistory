for (i in 1:nc){
zmin<-normDat$minAndMax[i,1]
zmax<-normDat$minAndMax[i,2]
X[,i]<-(normDat$dat[,i]+1)/2*(zmax-zmin)+zmin
}
return(X)
}
fun<-quadratic
d<-2
LB <- rep(-5,d)
UB <- rep(5,d)
ntrain <- 15
ntest <- 100
ndata <- ntrain + ntest
set.seed(1) # with this seed you can reproduce the data
rawX <-t(replicate(n = ndata,expr = runif(n = d,min = LB,max = UB)))
set.seed(Sys.time()) # unset seed, back to "random"
rawYobs <- apply(X = rawX,MARGIN = 1,FUN = fun)
# normalize the data between -1 and 1
X <- normByRow(rawX)
# you can recover unnormalized data with, for expl : X <- unnormByRow(normIn)
Yobs <- normByRow(as.matrix(rawYobs))
itrain <- 1:ntrain
itest <- (ntrain+1):ndata
# data in stats are stored as 1 row for 1 point.
# In the neural network world, like often in linear algebra, vectors are columns thus a transpose is needed
Xtrain<-t(X$dat[itrain,])
Xtest<-t(X$dat[itest,])
Ytrain<-t(matrix(data=Yobs$dat[itrain,],ncol=1))
Ytest<-t(matrix(data=Yobs$dat[itest,],ncol=1))
# Example of loss calculation
layerSizes <- c(2,4,1)
actFuncByLayer <- list(c(sigmoid, sigmoid, relu, relu), c(linear))
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
squareLoss(x)
source('utilities_optim.R')
source('line_searches.R')
source('gradient_descent.R')
source('restarted_descent.R')
pbFormulation <- list()
pbFormulation$fun<-squareLoss #function to minimize
d<-length(x)
pbFormulation$d<-d # dimension
pbFormulation$LB<-rep(-10,d) #lower bounds
pbFormulation$UB<-rep(10,d) #upper bounds
### algorithm settings
optAlgoParam <- list()
set.seed(112233) # repeatable run despite the use of pseudo-random number generators in runif
#
optAlgoParam$budget <- 4000
optAlgoParam$minGradNorm <- 1.e-6
optAlgoParam$minStepSize <- 1.e-11
optAlgoParam$nb_restarts <- 4
#
optAlgoParam$direction_type <- "momentum"
optAlgoParam$linesearch_type <- "armijo"
optAlgoParam$stepFactor <- 0.1 # step factor when there is no line search,
optAlgoParam$beta <- 0.9 # momentum term for direction_type == "momentum" or "NAG"
optAlgoParam$xinit <- weightsByLayerToX(weightsByLayer = Weights)
#
printlevel <- 2 # controls how much is stored and printed, choices: 0 to 4, cf. gradient_descent.R top comments for more info.
# a restarted descent
res <- restarted_descent(pbFormulation=pbFormulation,algoParam = optAlgoParam,printlevel=printlevel)
# extract the net learned
xBest <- res$xbest
lossBest <- res$fbest
wBest <- xtoWeightsByLayer(xBest,layerSizes)
# Performance on the training set
predTrain <- forwardPropagation(Xtrain, wBest, actFuncByLayer) # predictions of the NN learned
rmseTrain <- sqrt((sum((Ytrain-predTrain)^2))/ntrain)
cat("Training RMSE =",rmseTrain,"\n")
plot(Ytrain,predTrain,xlab="data",ylab="prediction",main="training",pch=3,col="blue")
ymin <- min(Ytrain)
ymax <- max(Ytrain)
lines(x = c(ymin,ymax), y=c(ymin,ymax))
# Performance on the testing set
predTest <- forwardPropagation(Xtest, wBest, actFuncByLayer) # predictions of the NN learned
rmseTest <- sqrt((sum((Ytest-predTest)^2))/ntest)
cat("Test RMSE =",rmseTest,"\n")
plot(Ytest,predTest,xlab="data",ylab="prediction",main="test",pch=3,col="red")
ymin <- min(Ytest)
ymax <- max(Ytest)
lines(x = c(ymin,ymax), y=c(ymin,ymax))
optAlgoParam$direction_type <- "momentum"
optAlgoParam$linesearch_type <- "armijo"
optAlgoParam$stepFactor <- 1 # step factor when there is no line search,
optAlgoParam$beta <- 0.5 # momentum term for direction_type == "momentum" or "NAG"
# a gradient descent
res_grad <- gradient_descent(pbFormulation=pbFormulation,algoParam = optAlgoParam,printlevel=printlevel)
# extract the net learned
xBest_grad <- res_grad$xbest
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad,layerSizes)
wBest_grad
# Performance sur l'ensemble d'entraînement
predTrain <- forwardPropagation(Xtrain, wBest_grad, actFuncByLayer) # Prédictions du réseau appris
rmseTrain <- sqrt((sum((Ytrain - predTrain)^2)) / ntrain)
cat("RMSE Entraînement =", rmseTrain, "\n")
plot(Ytrain, predTrain, xlab = "Données", ylab = "Prédictions", main = "Entraînement", pch = 3, col = "blue")
ymin <- min(Ytrain)
ymax <- max(Ytrain)
lines(x = c(ymin, ymax), y = c(ymin, ymax))
# Performance sur l'ensemble de test
predTest <- forwardPropagation(Xtest, wBest_grad, actFuncByLayer)
# Prédictions du réseau appris
rmseTest <- sqrt((sum((Ytest - predTest)^2)) / ntest)
cat("RMSE Test =", rmseTest, "\n")
plot(Ytest, predTest, xlab = "Données", ylab = "Prédictions", main = "Test", pch = 3, col = "red")
ymin <- min(Ytest)
ymax <- max(Ytest)
lines(x = c(ymin, ymax), y = c(ymin, ymax))
# Définir les grilles d'hyperparamètres
direction_types <- c("gradient", "momentum")#, "NAG")
linesearch_types <- c("none", "armijo")
stepFactors <- c(0.01,0.5,1) # Exemple de plage pour stepFactor
betas <- c(0.1,0.5,1) # Exemple de plage pour beta
# Initialisation de optAlgoParam comme liste
optAlgoParam <- list()
# Structure pour stocker les résultats
results <- data.frame(Direction = character(),
LineSearch = character(),
StepFactor = numeric(),
Beta = numeric(),
RMSETrain = numeric(),
RMSETest = numeric())
# Fonction pour évaluer les performances
evaluate_performance <- function(X, Y, weights, actFunc) {
pred <- forwardPropagation(X, weights, actFunc)
rmse <- sqrt(sum((Y - pred)^2) / length(Y))
return(rmse) }
# Boucle pour la recherche par grille
for (direction in direction_types) {
for (linesearch in linesearch_types) {
for (stepFactor in stepFactors) {
for (beta in betas) {
optAlgoParam$budget <- 4000
optAlgoParam$minGradNorm <- 1.e-6
optAlgoParam$minStepSize <- 1.e-11
optAlgoParam$nb_restarts <- 4
optAlgoParam$xinit <- weightsByLayerToX(weightsByLayer = Weights)
#
printlevel <- 2 # controls how much is stored and printed, choices: 0 to 4, cf. gradient_descent.R top comments for more info.
optAlgoParam$direction_type <- direction
optAlgoParam$linesearch_type <- linesearch
optAlgoParam$stepFactor <- stepFactor
optAlgoParam$beta <- beta
# Processus d'optimisation
# Assurez-vous que la fonction gradient_descent et toutes les fonctions connexes sont correctement définies
res_grad <- gradient_descent(pbFormulation, optAlgoParam, printlevel)
# Extract and evaluate results
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
# Évaluation sur l'ensemble d'entraînement
rmseTrain <- evaluate_performance(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
# Évaluation sur l'ensemble de test
rmseTest <- evaluate_performance(Xtest, Ytest, wBest_grad, actFuncByLayer)
# Enregistrer les résultats
results <- rbind(results, c(direction, linesearch, stepFactor, beta, rmseTrain, rmseTest))
}
}
}
}
# Afficher les résultats
print(results)
create_mini_batches <- function(X, Y, batch_size) {
n <- dim(X)[2]
indices <- sample(1:n, n)
num_batches <- ceiling(n / batch_size)
mini_batches <- list()
for (i in 1:num_batches) {
start_index <- (i - 1) * batch_size + 1
end_index <- min(i * batch_size, n)
batch_indices <- indices[start_index:end_index]
X_batch <- X[, batch_indices]
Y_batch <- Y[, batch_indices]
mini_batches[[i]] <- list(X_batch, Y_batch)
}
return(mini_batches)
}
### algorithm settings
optAlgoParam <- list()
set.seed(112233) # repeatable run despite the use of pseudo-random number generators in runif
#
optAlgoParam$budget <- 4000
optAlgoParam$minGradNorm <- 1.e-6
optAlgoParam$minStepSize <- 1.e-11
optAlgoParam$nb_restarts <- 4
#
optAlgoParam$direction_type <- "momentum"
optAlgoParam$linesearch_type <- "armijo"
optAlgoParam$stepFactor <- 1 # step factor when there is no line search,
optAlgoParam$beta <- 0.5 # momentum term for direction_type == "momentum" or "NAG"
optAlgoParam$xinit <- weightsByLayerToX(weightsByLayer = Weights)
printlevel <- 2 # controls how much is stored and printed, choices: 0 to 4, cf. gradient_descent.R top comments for more info.
batch_sizes <- c(5, ntrain)  # Différentes tailles de mini-batches à tester
# Boucle sur les différentes tailles de mini-batches
for (batch_size in batch_sizes) {
print(batch_size)
cat(sprintf("Entraînement avec batch size: %d\n", batch_size))
# Création des mini-batches
mini_batches <- create_mini_batches(Xtrain, Ytrain, batch_size)
# Initialisation des poids - Ajustez selon votre contexte
weights <- createRandomWeightsByLayer(layerSizes)
# Boucle sur les mini-batches pour l'entraînement
for (batch in mini_batches) {
X_batch <- batch[[1]]
Y_batch <- batch[[2]]
# Mise à jour des dataInputs et dataOutputs pour chaque mini-batch
dataInputs <- X_batch
dataOutputs <- Y_batch
# Appel de la descente de gradient pour chaque mini-batch
res_grad <- gradient_descent(pbFormulation, optAlgoParam, printlevel)
# Mise à jour des poids - Assurez-vous que votre fonction gradient_descent retourne les nouveaux poids
weights <- xtoWeightsByLayer(res_grad$xbest, layerSizes)
}
print(1)
# Évaluation des performances après une époque
performance <- evaluate_performance(Xtest, Ytest, weights, actFuncByLayer)
# Affichage des performances
cat(sprintf("Performance avec batch size %d: %f\n", batch_size, performance))
}
squareLoss<- function(x) {
weights <- xtoWeightsByLayer(x, layerSizes = layerSizes)
pred <- forwardPropagation(inputs = dataInputs, weightsByLayer = weights, activationFunction = actFuncByLayer)
# Assurer que les prédictions sont comprises entre un petit epsilon et 1 - epsilon pour éviter le log(0)
epsilon <- 1e-10
pred <- pmax(pmin(pred, 1 - epsilon), epsilon)
# Calcul de la cross-entropie
return(-mean(dataOutputs * log(pred) + (1 - dataOutputs) * log(1 - pred)))
}
threshold <- 0.5  # This is an example threshold
# Apply the rule
Ytrain <- ifelse(abs(Xtrain) > threshold, 1, 0)
Ytest <- ifelse(abs(Xtest) > threshold, 1, 0)
# Convert to the same format as your original Ytrain and Ytest
Ytrain <- t(matrix(data=Ytrain, ncol=1))
Ytest <- t(matrix(data=Ytest, ncol=1))
Ytrain <- Ytrain[, 1:15, drop = FALSE]
Ytest <- Ytest[, 1:100, drop = FALSE]
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(relu, tanh, relu, tanh), c(sigmoid) )
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=2)
# Extraction du réseau appris
xBest_grad <- res_grad$xbest
print(xBest_grad)
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
evaluate_performance_class <- function(X, Y, weights, actFunc) {
pred <- forwardPropagation(X, weights, actFunc)
predictedClasses <- ifelse(pred > 0.5, 1, 0)  # Convertir les prédictions en classes 0 ou 1
accuracy <- sum(predictedClasses == Y) / length(Y)  # Calcul de la précision
print("trueClasses")
print(Y)
print("predictedClasses")
print(predictedClasses)
return(accuracy)
}
# Évaluation de la performance sur l'ensemble d'entraînement
accuracyTrain <- evaluate_performance_class(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
cat("Précision Entraînement =", accuracyTrain, "\n")
# Évaluation de la performance sur l'ensemble de test
accuracyTest <- evaluate_performance_class(Xtest, Ytest, wBest_grad, actFuncByLayer)
cat("Précision Test =", accuracyTest, "\n")
# ... [Previous code for setting up the network, initializing weights, etc.]
# Hyperparameter Grids
direction_types <- c("gradient")
linesearch_types <- c("none", "armijo")
stepFactors <- c(0.001, 0.005, 0.01)
betas <- c(0.1, 0.5, 1)
# Initialize optAlgoParam as a list
optAlgoParam <- list()
# Structure to store the results
results <- data.frame(Direction = character(),
LineSearch = character(),
StepFactor = numeric(),
Beta = numeric(),
RMSETrain = numeric(),
RMSETest = numeric())
# Grid Search Loop
for (direction in direction_types) {
for (linesearch in linesearch_types) {
for (stepFactor in stepFactors) {
for (beta in betas) {
# Set hyperparameters
optAlgoParam$direction_type <- direction
optAlgoParam$linesearch_type <- linesearch
optAlgoParam$stepFactor <- stepFactor
optAlgoParam$beta <- beta
optAlgoParam$xinit <- weightsByLayerToX(weightsByLayer = Weights)
# Other parameters like budget, minGradNorm, etc.
# Run gradient descent
res_grad <- gradient_descent(pbFormulation, optAlgoParam, printlevel)
# Extract and evaluate results
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
perfTrain <- evaluate_performance_class(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
perfTest <- evaluate_performance_class(Xtest, Ytest, wBest_grad, actFuncByLayer)
# Store results
results <- rbind(results, c(direction, linesearch, stepFactor, beta, perfTrain, perfTest))
}
}
}
}
squareLoss<- function(x) {
weights <- xtoWeightsByLayer(x, layerSizes = layerSizes)
pred <- forwardPropagation(inputs = dataInputs, weightsByLayer = weights, activationFunction = actFuncByLayer)
# Assurer que les prédictions sont comprises entre un petit epsilon et 1 - epsilon pour éviter le log(0)
epsilon <- 1e-10
pred <- pmax(pmin(pred, 1 - epsilon), epsilon)
# Calcul de la cross-entropie
return(-mean(dataOutputs * log(pred) + (1 - dataOutputs) * log(1 - pred)))
}
squareLoss<- function(x) {
return("azeaeifndkvnn")
}
threshold <- 0.5  # This is an example threshold
# Apply the rule
Ytrain <- ifelse(abs(Xtrain) > threshold, 1, 0)
Ytest <- ifelse(abs(Xtest) > threshold, 1, 0)
# Convert to the same format as your original Ytrain and Ytest
Ytrain <- t(matrix(data=Ytrain, ncol=1))
Ytest <- t(matrix(data=Ytest, ncol=1))
Ytrain <- Ytrain[, 1:15, drop = FALSE]
Ytest <- Ytest[, 1:100, drop = FALSE]
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(relu, tanh, relu, tanh), c(sigmoid) )
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=2)
# Extraction du réseau appris
xBest_grad <- res_grad$xbest
print(xBest_grad)
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
evaluate_performance_class <- function(X, Y, weights, actFunc) {
pred <- forwardPropagation(X, weights, actFunc)
predictedClasses <- ifelse(pred > 0.5, 1, 0)  # Convertir les prédictions en classes 0 ou 1
accuracy <- sum(predictedClasses == Y) / length(Y)  # Calcul de la précision
print("trueClasses")
print(Y)
print("predictedClasses")
print(predictedClasses)
return(accuracy)
}
# Évaluation de la performance sur l'ensemble d'entraînement
accuracyTrain <- evaluate_performance_class(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
cat("Précision Entraînement =", accuracyTrain, "\n")
# Évaluation de la performance sur l'ensemble de test
accuracyTest <- evaluate_performance_class(Xtest, Ytest, wBest_grad, actFuncByLayer)
cat("Précision Test =", accuracyTest, "\n")
squareLoss<- function(x) {
weights <- xtoWeightsByLayer(x, layerSizes = layerSizes)
pred <- forwardPropagation(inputs = dataInputs, weightsByLayer = weights, activationFunction = actFuncByLayer)
# Assurer que les prédictions sont comprises entre un petit epsilon et 1 - epsilon pour éviter le log(0)
epsilon <- 1e-10
pred <- pmax(pmin(pred, 1 - epsilon), epsilon)
# Calcul de la cross-entropie
return(-mean(dataOutputs * log(pred) + (1 - dataOutputs) * log(1 - pred)))
}
threshold <- 0.5  # This is an example threshold
# Apply the rule
Ytrain <- ifelse(abs(Xtrain) > threshold, 1, 0)
Ytest <- ifelse(abs(Xtest) > threshold, 1, 0)
# Convert to the same format as your original Ytrain and Ytest
Ytrain <- t(matrix(data=Ytrain, ncol=1))
Ytest <- t(matrix(data=Ytest, ncol=1))
Ytrain <- Ytrain[, 1:15, drop = FALSE]
Ytest <- Ytest[, 1:100, drop = FALSE]
pbFormulation <- list()
pbFormulation$fun<-squareLoss #function to minimize
d<-length(x)
pbFormulation$d<-d # dimension
pbFormulation$LB<-rep(-10,d) #lower bounds
pbFormulation$UB<-rep(10,d) #upper bounds
### algorithm settings
optAlgoParam <- list()
set.seed(112233) # repeatable run despite the use of pseudo-random number generators in runif
#
optAlgoParam$budget <- 4000
optAlgoParam$minGradNorm <- 1.e-6
optAlgoParam$minStepSize <- 1.e-11
optAlgoParam$nb_restarts <- 4
#
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
optAlgoParam$xinit <- weightsByLayerToX(weightsByLayer = Weights)
#
printlevel <- 2 # controls how much is stored and printed, choices: 0 to 4, cf. gradient_descent.R top comments for more info.
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(relu, tanh, relu, tanh), c(sigmoid) )
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=2)
# Extraction du réseau appris
xBest_grad <- res_grad$xbest
print(xBest_grad)
lossBest_grad <- res_grad$fbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
xBest_grad <- res_grad$xbest
wBest_grad <- xtoWeightsByLayer(xBest_grad, layerSizes)
evaluate_performance_class <- function(X, Y, weights, actFunc) {
pred <- forwardPropagation(X, weights, actFunc)
predictedClasses <- ifelse(pred > 0.5, 1, 0)  # Convertir les prédictions en classes 0 ou 1
accuracy <- sum(predictedClasses == Y) / length(Y)  # Calcul de la précision
print("trueClasses")
print(Y)
print("predictedClasses")
print(predictedClasses)
return(accuracy)
}
# Évaluation de la performance sur l'ensemble d'entraînement
accuracyTrain <- evaluate_performance_class(Xtrain, Ytrain, wBest_grad, actFuncByLayer)
cat("Précision Entraînement =", accuracyTrain, "\n")
# Évaluation de la performance sur l'ensemble de test
accuracyTest <- evaluate_performance_class(Xtest, Ytest, wBest_grad, actFuncByLayer)
cat("Précision Test =", accuracyTest, "\n")
squareLoss<- function(x) {
weights <- xtoWeightsByLayer(x, layerSizes = layerSizes)
pred <- forwardPropagation(inputs = dataInputs, weightsByLayer = weights, activationFunction = actFuncByLayer)
# Assurer que les prédictions sont comprises entre un petit epsilon et 1 - epsilon pour éviter le log(0)
epsilon <- 1e-10
pred <- pmax(pmin(pred, 1 - epsilon), epsilon)
# Calcul de la cross-entropie
return(-mean(dataOutputs * log(pred) + (1 - dataOutputs) * log(1 - pred)))
}
squareLoss<- function(x) {
return( "penis")
}
threshold <- 0.5  # This is an example threshold
# Apply the rule
Ytrain <- ifelse(abs(Xtrain) > threshold, 1, 0)
Ytest <- ifelse(abs(Xtest) > threshold, 1, 0)
# Convert to the same format as your original Ytrain and Ytest
Ytrain <- t(matrix(data=Ytrain, ncol=1))
Ytest <- t(matrix(data=Ytest, ncol=1))
Ytrain <- Ytrain[, 1:15, drop = FALSE]
Ytest <- Ytest[, 1:100, drop = FALSE]
pbFormulation <- list()
pbFormulation$fun<-squareLoss #function to minimize
d<-length(x)
pbFormulation$d<-d # dimension
pbFormulation$LB<-rep(-10,d) #lower bounds
pbFormulation$UB<-rep(10,d) #upper bounds
### algorithm settings
optAlgoParam <- list()
set.seed(112233) # repeatable run despite the use of pseudo-random number generators in runif
#
optAlgoParam$budget <- 4000
optAlgoParam$minGradNorm <- 1.e-6
optAlgoParam$minStepSize <- 1.e-11
optAlgoParam$nb_restarts <- 4
#
# Paramètres pour l'algorithme de descente de gradient
optAlgoParam$direction_type <- "gradient"
optAlgoParam$linesearch_type <- "none"
optAlgoParam$stepFactor <- 0.01
optAlgoParam$beta <- 0.1
optAlgoParam$xinit <- weightsByLayerToX(weightsByLayer = Weights)
#
printlevel <- 2 # controls how much is stored and printed, choices: 0 to 4, cf. gradient_descent.R top comments for more info.
# Initialisation des poids et préparation des données
set.seed(5678)
Weights <- createRandomWeightsByLayer(layerSizes)
dataInputs <- Xtrain
dataOutputs <- Ytrain
x <- weightsByLayerToX(weightsByLayer = Weights)
layerSizes <- c(2, 4, 1)  # Pour la classification binaire, la dernière couche doit avoir 1 neurone
# ReLU dans les couches cachées, Sigmoïde dans la couche de sortie pour la classification binaire
actFuncByLayer <- list(c(relu, tanh, relu, tanh), c(sigmoid) )
# Exécution de la descente de gradient
res_grad <- gradient_descent(pbFormulation=pbFormulation, algoParam = optAlgoParam, printlevel=2)
